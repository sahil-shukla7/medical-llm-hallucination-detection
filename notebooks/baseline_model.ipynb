{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggcCOS5xSSFW"
      },
      "source": [
        "# Baseline Model Notebook\n",
        "This notebook loads the PubMedQA medical QA dataset and runs the baseline medical-LLM inference."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0uWN-kOR5d7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval-Augmented Generation (RAG) Experiment\n",
        "\n",
        "\n",
        "## Project Overview:\n",
        "\n",
        "This notebook demonstrates the implementation of a basic RAG pipeline using Hugging Face models and external knowledge sources.  \n",
        "The model is tested with and without retrieval to observe hallucination behavior.\n",
        "\n",
        "\n",
        "## Notebook Outline:\n",
        "\n",
        "**Importing Libraries**  \n",
        "Initialization of required modules such as Transformer models, retrievers, and utility functions.  \n",
        "\n",
        "\n",
        "**Loading Model & Tokenizer**  \n",
        "Preparing the language model for generation.  \n",
        "\n",
        "\n",
        "**Wikipedia Retriever Setup**  \n",
        "Connecting a retriever to fetch relevant real-world information.  \n",
        "\n",
        "\n",
        "**Baseline Model Response (Without Retrieval)**  \n",
        "Generating a response directly from the model to observe hallucinations.  \n",
        "\n",
        "\n",
        "**Retrieved Context + Model Response (With RAG)**  \n",
        "Generating a grounded response using external retrieved data.  \n",
        "\n",
        "\n",
        "**Observation:**  \n",
        "Comparing both outputs to analyze whether hallucination is reduced.  \n",
        "\n",
        "\n",
        "**DuckDuckGo Retriever Attempt (Optional â€“ Not Used in Final Output)**  \n",
        "This section shows an attempt to fetch web search results using a secondary retriever.  \n",
        "Due to API behavior and result limitations, this is not part of the final evaluation, but kept to demonstrate experimentation.  \n",
        "\n",
        "\n",
        "## Objective:\n",
        "\n",
        "To compare model responses with vs. without external retrieval and identify cases of hallucination, demonstrating how RAG improves factual accuracy.\n"
      ],
      "metadata": {
        "id": "i63bUcoQhjje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers sentencepiece accelerate wikipedia\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "dzyAjVSASdjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n"
      ],
      "metadata": {
        "id": "Vk1h7rTDS_jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_pipeline = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=\"dmis-lab/biobert-base-cased-v1.1\",\n",
        "    tokenizer=\"dmis-lab/biobert-base-cased-v1.1\"\n",
        ")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EzSPb7ZRUI0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_answer = qa_pipeline({\n",
        "    \"question\": \"Can antibiotics help treat a viral infection?\",\n",
        "    \"context\": \"Antibiotics are medications designed to treat bacterial infections. They do not work against viruses, such as the common cold or flu.\"\n",
        "})\n",
        "\n",
        "baseline_answer\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "byFdFnCmUtTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "pprint.pprint(baseline_answer)\n",
        "\n",
        "print(\"Answer:\", baseline_answer['answer'])\n",
        "print(\"Confidence:\", round(baseline_answer['score'], 3))\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "B_6XtI-AU-c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hallucinated_answer = qa_pipeline({\n",
        "    \"question\": \"Can antibiotics help treat a viral infection?\",\n",
        "    \"context\": \"The capital of France is Paris. Tigers are carnivores and live in forests. The sun is a star.\"\n",
        "})\n",
        "\n",
        "hallucinated_answer\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EqL5bLxlVuaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hallucinated_answer = qa_pipeline({\n",
        "    \"question\": \"Can antibiotics help treat a viral infection?\",\n",
        "    \"context\": \"The capital of France is Paris. Tigers are carnivores and live in forests. The sun is a star.\"\n",
        "})\n",
        "\n",
        "print(\"\\n--- Hallucinated Model Output ---\")\n",
        "print(\"Answer:\", hallucinated_answer['answer'])\n",
        "print(\"Confidence:\", round(hallucinated_answer['score'], 3))\n",
        "print(\"Start Index:\", hallucinated_answer['start'])\n",
        "print(\"End Index:\", hallucinated_answer['end'])\n",
        "\n",
        "context = \"The capital of France is Paris. Tigers are carnivores and live in forests. The sun is a star.\"\n",
        "\n",
        "print(\"\\nExtracted Span from Context:\")\n",
        "print(context[hallucinated_answer['start']:hallucinated_answer['end']])\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "nsfPsF9fXPJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation**\n",
        "\n",
        "The model attempted to generate an answer even when the provided context contained no medically relevant information. This behavior demonstrates a form of hallucination, where the model produces text without factual grounding. Although the hallucinated response was incorrect, the confidence score was noticeably lower compared to the response generated using correct context. This suggests that confidence can act as a useful signal for detecting hallucinations in medical LLMs."
      ],
      "metadata": {
        "id": "sqoqYbYEYA1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install duckduckgo-search\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vlzBFXIGYQ0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from duckduckgo_search import DDGS\n",
        "\n",
        "query = \"Do antibiotics work for viral infections PubMed medical research\"\n",
        "search_results = list(DDGS().text(query, max_results=3))\n",
        "\n",
        "search_results\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JL2TfNPxdpV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "VgE0_LdId0l5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "\n",
        "query = \"Antibiotics viral infection\"\n",
        "result_text = wikipedia.summary(\"Antibiotic misuse\", sentences=3)\n",
        "result_text\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "e1mnachyd_qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Summary of Retrieved Information\n",
        "\n",
        "The following text was fetched using the Wikipedia library based on the query *\"Antibiotics viral infection\"*.\n"
      ],
      "metadata": {
        "id": "9GOUXnpYeqhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(result_text)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "hCmcrO5gestN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_answer = qa_pipeline({\n",
        "    \"question\": \"Can antibiotics help treat a viral infection?\",\n",
        "    \"context\": result_text\n",
        "})\n",
        "\n",
        "rag_answer\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "aJMfcBc6e4RZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation**\n",
        "\n",
        "The model generated an answer even though the context did not contain relevant medical information.\n",
        "The extracted span is unrelated to the question, and the confidence score is extremely low, indicating uncertainty.\n",
        "This behavior demonstrates a hallucination, where the model produces an answer despite insufficient or irrelevant context.\n",
        "\n",
        "**Expected Behavior**\n",
        "\n",
        "The ideal response would have been:\n",
        "\n",
        "\"The context does not contain enough information to answer the question.\""
      ],
      "metadata": {
        "id": "0PhN2jrQfouU"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}